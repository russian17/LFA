# Lexer & Scanner
### Course: Formal Languages & Finite Automata
### Author: Cuzmin Simion
----

## Theory
Lexical tokenization is conversion of a text into (semantically or syntactically) meaningful lexical tokens belonging to categories defined by a "lexer" program. In case of a natural language, those categories include nouns, verbs, adjectives, punctuations etc. In case of a programming language, the categories include identifiers, operators, grouping symbols and data types. Lexical tokenization is related to the type of tokenization used in Large language models (LLMs), but with two differences. First, lexical tokenization is usually based on a lexical grammar, whereas LLM tokenizers are usually probability-based. Second, LLM tokenizers perform a second step that converts the tokens into numerical values.
A rule-based program, performing lexical tokenization, is called tokenizer,[1] or scanner, although scanner is also a term for the first stage of a lexer. A lexer forms the first phase of a compiler frontend in processing. Analysis generally occurs in one pass. Lexers and parsers are most often used for compilers, but can be used for other computer language tools, such as prettyprinters or linters. Lexing can be divided into two stages: the scanning, which segments the input string into syntactic units called lexemes and categorizes these into token classes; and the evaluating, which converts lexemes into processed values.

Lexers are generally quite simple, with most of the complexity deferred to the syntactic analysis or semantic analysis phases, and can often be generated by a lexer generator, notably lex or derivatives. However, lexers can sometimes include some complexity, such as phrase structure processing to make input easier and simplify the parser, and may be written partly or fully by hand, either to support more features or for performance.
## Objectives:
1. Understand what lexical analysis [1] is.
2. Get familiar with the inner workings of a lexer/scanner/tokenizer.
3. Implement a sample lexer and show how it works.

## Implementation description
### 1. my_token.py
  my_token.py serves as the foundation for defining the lexical components of a programming language in the lexer framework. It outlines the structure of tokens, which are the basic units of meaning within the source code, and enumerates the types of tokens that the lexer will recognize. This file effectively encapsulates the vocabulary of the target programming language.
Token types such as `TT_INT` for integers, `TT_FLOAT` for floating-point numbers, `TT_PLUS` for the plus operator, and many others that categorize the various syntactic elements like identifiers, keywords, literals, and operators.
`DIGITS` and `KEYWORDS` constants to help identify numeric literals and reserved words in the language.

## Token class:
Attributes:
- `type`: Token type
- `value`: Value of token


### 2. lexer.py
lexer.py contains the implementation of the Lexer class, which is responsible for converting the input source code into a sequence of tokens defined in my_token.py. This process involves scanning the source code character by character, grouping these characters into meaningful tokens, and handling special cases such as comments and whitespace.

## Position class:

Attributes:
- `idx`: index
- `ln`: Line number
- `col`: Column number
- `fn`: File name
- `ftxt`: Full text of the file

Methods:
- `advance`: Helper function which moves to next charachter
- `copy`: Helper function which copies the current position
  

## Lexer class:

Attributes:
- `fn`: File name 
- `text`: Line number
- `pos`: # Starting position
- `current_char`: Current character
- `advance`: Helper function which moves to next charachter

Methods:
- `advance`: Helper function which moves to next charachter
- `peek`: Helper function which peeks at the next character.
- `make_tokens`: Lexing the input text
- `make_number`:  Making a number token
- `make_string`:  Making a string token
- `make_identifier`: Making a identifier token
- `skip_comment`: Skipping comments

Methods:
- `run`: Running the lexer

### 3. test.py

The primary goal of test.py is to serve as a manual testing tool for the lexer. By providing sample input and examining the output, developers can:

Identify Errors: Detect issues where the lexer fails to correctly identify certain language constructs or generates incorrect token types.

Verify Tokenization: Ensure that the lexer correctly breaks down the input text into tokens, including accurately identifying the boundaries between tokens.

Validate Handling of Edge Cases: Test how the lexer deals with edge cases, such as strings containing escaped characters, numbers with various formats, and comments.

In summary, test.py is an essential component of the lexer development process, providing a straightforward and effective way to manually test the lexer's functionality. By allowing for quick feedback on the lexer's performance with various inputs, it aids in the iterative development and refinement of the lexer.

## Results
#### test.py:
![image](https://github.com/russian17/LFA/assets/120457811/6131f3da-5246-45e8-a164-23881a331403)

![image](https://github.com/russian17/LFA/assets/120457811/77d91299-e662-4d63-bce6-35420244aef5)
![image](https://github.com/russian17/LFA/assets/120457811/0a06faa9-2419-4fdb-a97e-5ae2f1d30ce6)



## Conclusions
In conclusion, the development showcased through my_token.py, lexer.py, and test.py provides a comprehensive look into constructing a lexer, an essential component of compiler design and language processing. This work demonstrates the critical process of breaking down source code into tokens, which form the backbone for further analysis in the compilation process.

The lexer's architecture detailed in lexer.py highlights a sophisticated system for categorizing various elements of programming languages into structured tokens, as defined in my_token.py. This categorization is crucial for transforming raw text into a meaningful sequence that can be more easily analyzed in subsequent stages of compilation or interpretation.

The test.py script plays an indispensable role in validating the lexer's functionality, enabling iterative refinement through practical input-output testing. This process ensures that the lexer accurately recognizes language constructs and adheres to the target language's specifications.

Overall, this work emphasizes the lexer's vital role in translating human-readable code into a format that paves the way for machine execution, underscoring the importance of lexical analysis in the broader context of computing and software development.





